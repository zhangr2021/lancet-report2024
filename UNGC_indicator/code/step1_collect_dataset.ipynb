{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a2d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f2d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# company info\n",
    "def page_participation(page = 1):\n",
    "    url = \"https://unglobalcompact.org/what-is-gc/participants?page=\" + str(page)\n",
    "    # Requests URL and get response object\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse text obtained\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all hyperlinks present on webpage\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    # From all links check for pdf link and\n",
    "    # if present download file\n",
    "    df = pd.DataFrame()\n",
    "    for link in links:\n",
    "        if ('/what-is-gc/participants/' in link.get('href', [])):\n",
    "            link_company = \"https://unglobalcompact.org\" + link.get('href')\n",
    "            page = requests.get(link_company)\n",
    "            # Create a BeautifulSoup object\n",
    "            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "            title = soup.find_all(\"title\")[0].text\n",
    "            cat = [i.text.replace(\":\", \"\") for i in soup.find_all(\"dt\")]\n",
    "            para = [i.text for i in soup.find_all(\"dd\")]\n",
    "            tmp = pd.DataFrame(para).T\n",
    "            tmp.columns = cat\n",
    "            tmp[\"link\"] = link_company\n",
    "            df = pd.concat([df, tmp])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ = page_participation(page = 1)\n",
    "for page in range(610, 2029):\n",
    "    tmp_df = page_participation(page = page)\n",
    "    tmp_df[\"page\"] = page\n",
    "    df_ = pd.concat([df_, tmp_df])\n",
    "    df_.to_csv(\"company_scrape.csv\")\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv(\"./data/company_scrape.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_init = set(df_[\"link\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58253c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_links(links_comp, url_init, Country, Sector):\n",
    "    df_pdfs = pd.DataFrame()\n",
    "    for i in range(len(links_comp)): \n",
    "        upper_link = links_comp[i].get(\"href\")\n",
    "        upper_content = links_comp[i].contents\n",
    "        if upper_link[:2] == \"//\":\n",
    "            url = upper_link.replace(\"//\", \"\")\n",
    "        elif \"http\" in upper_link:\n",
    "            url = upper_link\n",
    "        else:\n",
    "            url = \"https://unglobalcompact.org\" + upper_link\n",
    "        upper_link = url\n",
    "\n",
    "        if (\"communication\" in str(upper_content).lower()) or (\"report\" in str(upper_content).lower()) or (\"review\" in str(upper_content).lower()) or (\"cop\" in str(upper_content).lower()):\n",
    "            dct = {\"upper_link\": upper_link, \"upper_content\": upper_content, \"pdf_link\":[''],\"pdf_content\": [\"\"]}\n",
    "            #print(\"init url\", url, \"\\n\\ninit content \", upper_content)\n",
    "            try:\n",
    "                response = requests.get(upper_link)\n",
    "\n",
    "                # Parse text obtained\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                links  = soup.find_all('a')\n",
    "                lst_pdfs = []\n",
    "                lst_contents = []\n",
    "                for l in links:\n",
    "                    pdf_link = l.get(\"href\")\n",
    "                    content = l.contents\n",
    "                    try:\n",
    "                        if \"pdf\" in pdf_link:\n",
    "                            lst_pdfs.append(pdf_link)\n",
    "                            lst_contents.append(content)\n",
    "                    except: pass\n",
    "                dct[\"pdf_link\"] = lst_pdfs \n",
    "                dct[\"pdf_content\"] = content\n",
    "                tmp = pd.DataFrame(dct)\n",
    "                df_pdfs = pd.concat([df_pdfs, tmp])\n",
    "            except:\n",
    "                tmp = pd.DataFrame.from_dict(dct, orient='index').T\n",
    "                df_pdfs = pd.concat([df_pdfs, tmp])\n",
    "            df_pdfs[\"link_company\"] = url_init\n",
    "            df_pdfs[\"Country\"] = Country\n",
    "            df_pdfs[\"Sector\"] = Sector\n",
    "        else: pass #print(\"pass: \", url)\n",
    "    return df_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bdceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_link_done = pd.read_csv(\"./data/pdf_links_collection.csv\") # if any previous search has been saved\n",
    "missed = set(df_.link).difference(set(pdfs_link_done.link_company.unique()))\n",
    "missed_df = df_.set_index(\"link\").loc[list(missed)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs_link = pd.DataFrame()\n",
    "for i in range(len(missed_df)):\n",
    "    url_init, Country, Sector= missed_df.iloc[i*(-1)][[\"link\", \"Country\", \"Sector\"]]\n",
    "    response = requests.get(url_init)\n",
    "\n",
    "    # Parse text obtained\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all hyperlinks present on webpage\n",
    "    links_comp = soup.find_all('a')\n",
    "    tmp = retrieve_links(links_comp, url_init, Country, Sector)\n",
    "    pdfs_link = pd.concat([pdfs_link, tmp])\n",
    "    print(i)\n",
    "    if i%10 ==0:\n",
    "        pdfs_link.to_csv(\"pdf_links_collection_R2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df377bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISO3 = pd.read_csv(\"./data/iso_3.csv\", sep=\",\", encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pdfs_link[\"company_name\"] = pdfs_link.link_company.apply(lambda x: x.split(\"/\")[-1].replace(\"-\", \" \"))\n",
    "pdfs_link[\"company_id\"] = pdfs_link[\"company_name\"].apply(lambda x: x.split()[0])\n",
    "pdfs_link[\"company_id\"].nunique()\n",
    "pdfs_link[\"year\"] = pdfs_link.upper_content.apply(lambda x: re.findall(r'\\d+', str(x)))\n",
    "pdfs_link[\"year\"] = [int(y[0]) if y else None for y in pdfs_link[\"year\"]]\n",
    "pdfs_link[\"year_links\"] = pdfs_link.pdf_link.apply(lambda x: re.findall(r'\\d+', \" \".join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_2023 = pd.merge(target_2023, ISO3, left_on = \"Country\", right_on = \"country\", how = \"left\")\n",
    "target_2023[target_2023[\"code\"].isna()].Country.unique()\n",
    "target_2023[\"file_id\"] = target_2023.company_id.apply(lambda x: \"2023_\" + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2087562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def retrive_link(link, file_id, ISO, idx):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(link)\n",
    "    time.sleep(6) # any number > 3 should work fine\n",
    "    html = driver.page_source\n",
    "    \n",
    "    try:\n",
    "        prefix = 'https://cop-report.'\n",
    "        pdf_link = re.findall(prefix + '([\\w\\-\\./]+)', html) \n",
    "        response = requests.get(prefix + pdf_link[0])\n",
    "        filename = Path('pdf/2023/' + file_id + \"_\" + ISO + '.pdf')\n",
    "        filename.write_bytes(response.content)\n",
    "    except: \n",
    "        print(\"Error: \", file_id, link, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f3c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(target_2023)):\n",
    "    row = target_2023.iloc[idx]\n",
    "    \n",
    "    link = row[\"upper_link\"]\n",
    "    file_id = row[\"file_id\"]\n",
    "    ISO = row[\"code\"]\n",
    "    \n",
    "    response = requests.get(link)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links_comp = soup.find_all('a')\n",
    "    quota1 = False\n",
    "    try:\n",
    "        quota1 = links_comp[0].contents[0] == 'Reload'\n",
    "    except: \n",
    "        pass\n",
    "    if quota1 or (len(links_comp) == 0):\n",
    "        retrive_link(link, file_id, ISO, idx)\n",
    "    else:\n",
    "        for link in links_comp:\n",
    "            if ('.pdf' in link.get('href', [])):\n",
    "                print(\"Downloading file: \", idx)\n",
    "                if \"http\" in link.get('href'):\n",
    "                    response = requests.get(link.get('href'))\n",
    "                else:\n",
    "                    response = requests.get(\"http:\" + link.get('href'))\n",
    "\n",
    "                filename = Path('pdf/2023/' + file_id + \"_\" + ISO + '.pdf')\n",
    "                filename.write_bytes(response.content)\n",
    "                print(\"File \", idx, \" downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f84883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf to text\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# To read the PDF\n",
    "import PyPDF2\n",
    "# To analyze the PDF layout and extract text\n",
    "from pdfminer.high_level import extract_pages, extract_text\n",
    "from pdfminer.layout import LTTextContainer, LTChar, LTRect, LTFigure\n",
    "# To extract text from tables in PDF\n",
    "import pytesseract \n",
    "# To remove the additional created files\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02610ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to extract text\n",
    "import pdfplumber\n",
    "# To extract the images from the PDFs\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "# To perform OCR to extract text from images \n",
    "import pytesseract \n",
    "# To remove the additional created files\n",
    "import os\n",
    "\n",
    "def text_extraction(element):\n",
    "    # Extracting the text from the in-line text element\n",
    "    line_text = element.get_text()\n",
    "    \n",
    "    # Find the formats of the text\n",
    "    # Initialize the list with all the formats that appeared in the line of text\n",
    "    line_formats = []\n",
    "    for text_line in element:\n",
    "        if isinstance(text_line, LTTextContainer):\n",
    "            # Iterating through each character in the line of text\n",
    "            for character in text_line:\n",
    "                if isinstance(character, LTChar):\n",
    "                    # Append the font name of the character\n",
    "                    line_formats.append(character.fontname)\n",
    "                    # Append the font size of the character\n",
    "                    line_formats.append(character.size)\n",
    "    # Find the unique font sizes and names in the line\n",
    "    format_per_line = list(set(line_formats))\n",
    "    \n",
    "    # Return a tuple with the text in each line along with its format\n",
    "    return (line_text, format_per_line)\n",
    "\n",
    "# Create a function to crop the image elements from PDFs\n",
    "def crop_image(element, pageObj):\n",
    "    # Get the coordinates to crop the image from the PDF\n",
    "    [image_left, image_top, image_right, image_bottom] = [element.x0,element.y0,element.x1,element.y1] \n",
    "    # Crop the page using coordinates (left, bottom, right, top)\n",
    "    pageObj.mediabox.lower_left = (image_left, image_bottom)\n",
    "    pageObj.mediabox.upper_right = (image_right, image_top)\n",
    "    # Save the cropped page to a new PDF\n",
    "    cropped_pdf_writer = PyPDF2.PdfWriter()\n",
    "    cropped_pdf_writer.add_page(pageObj)\n",
    "    # Save the cropped PDF to a new file\n",
    "    with open('cropped_image.pdf', 'wb') as cropped_pdf_file:\n",
    "        cropped_pdf_writer.write(cropped_pdf_file)\n",
    "\n",
    "# Create a function to convert the PDF to images\n",
    "def convert_to_images(input_file,):\n",
    "    images = convert_from_path(input_file)\n",
    "    image = images[0]\n",
    "    output_file = \"PDF_image.png\"\n",
    "    image.save(output_file, \"PNG\")\n",
    "\n",
    "# Create a function to read text from images\n",
    "def image_to_text(image_path):\n",
    "    # Read the image\n",
    "    img = Image.open(image_path)\n",
    "    # Extract the text from the image\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "# Extracting tables from the page\n",
    "\n",
    "def extract_table(pdf_path, page_num, table_num):\n",
    "    # Open the pdf file\n",
    "    pdf = pdfplumber.open(pdf_path)\n",
    "    # Find the examined page\n",
    "    table_page = pdf.pages[page_num]\n",
    "    # Extract the appropriate table\n",
    "    table = table_page.extract_tables()[table_num]\n",
    "    return table\n",
    "\n",
    "# Convert table into the appropriate format\n",
    "def table_converter(table):\n",
    "    table_string = ''\n",
    "    # Iterate through each row of the table\n",
    "    for row_num in range(len(table)):\n",
    "        row = table[row_num]\n",
    "        # Remove the line breaker from the wrapped texts\n",
    "        cleaned_row = [item.replace('\\n', ' ') if item is not None and '\\n' in item else 'None' if item is None else item for item in row]\n",
    "        # Convert the table into a string \n",
    "        table_string+=('|'+'|'.join(cleaned_row)+'|'+'\\n')\n",
    "    # Removing the last line break\n",
    "    table_string = table_string[:-1]\n",
    "    return table_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b29c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_txt(pdf_path):\n",
    "    # create a PDF file object\n",
    "    pdfFileObj = open(pdf_path, 'rb')\n",
    "    # create a PDF reader object\n",
    "    pdfReaded = PyPDF2.PdfReader(pdfFileObj)\n",
    "\n",
    "    # Create the dictionary to extract text from each image\n",
    "    text_per_page = {}\n",
    "    # We extract the pages from the PDF\n",
    "    for pagenum, page in enumerate(extract_pages(pdf_path)):\n",
    "        if pagenum%5 == 0:\n",
    "            print(pagenum)\n",
    "        # Initialize the variables needed for the text extraction from the page\n",
    "        pageObj = pdfReaded.pages[pagenum]\n",
    "        page_text = []\n",
    "        line_format = []\n",
    "        text_from_images = []\n",
    "        text_from_tables = []\n",
    "        page_content = []\n",
    "        # Initialize the number of the examined tables\n",
    "        table_num = 0\n",
    "        first_element= True\n",
    "        table_extraction_flag= False\n",
    "        # Open the pdf file\n",
    "        pdf = pdfplumber.open(pdf_path)\n",
    "        # Find the examined page\n",
    "        page_tables = pdf.pages[pagenum]\n",
    "        # Find the number of tables on the page\n",
    "        tables = page_tables.find_tables()\n",
    "\n",
    "\n",
    "        # Find all the elements\n",
    "        page_elements = [(element.y1, element) for element in page._objs]\n",
    "        # Sort all the elements as they appear in the page \n",
    "        page_elements.sort(key=lambda a: a[0], reverse=True)\n",
    "        \n",
    "        if_text = False\n",
    "        # Find the elements that composed a page\n",
    "        for i, component in enumerate(page_elements):\n",
    "            # Extract the position of the top side of the element in the PDF\n",
    "            pos= component[0]\n",
    "            # Extract the element of the page layout\n",
    "            element = component[1]\n",
    "            \n",
    "            # Check if the element is a text element\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                # Check if the text appeared in a table\n",
    "                if table_extraction_flag == False:\n",
    "                    # Use the function to extract the text and format for each text element\n",
    "                    (line_text, format_per_line) = text_extraction(element)\n",
    "                    # Append the text of each line to the page text\n",
    "                    page_text.append(line_text)\n",
    "                    # Append the format for each line containing text\n",
    "                    line_format.append(format_per_line)\n",
    "                    page_content.append(line_text)\n",
    "                else:\n",
    "                    # Omit the text that appeared in a table\n",
    "                    pass\n",
    "            #\n",
    "            # Check the elements for images\n",
    "            if (isinstance(element, LTFigure)) & (pagenum >2):\n",
    "                # Crop the image from the PDF\n",
    "                crop_image(element, pageObj)\n",
    "                # Convert the cropped pdf to an image\n",
    "                convert_to_images('cropped_image.pdf')\n",
    "                # Extract the text from the image\n",
    "                image_text = image_to_text('PDF_image.png')\n",
    "                text_from_images.append(image_text)\n",
    "                page_content.append(image_text)\n",
    "                # Add a placeholder in the text and format lists\n",
    "                page_text.append('image')\n",
    "                line_format.append('image')\n",
    "            \n",
    "        # Create the key of the dictionary\n",
    "        dctkey = 'Page_'+str(pagenum)\n",
    "        # Add the list of list as the value of the page key\n",
    "        text_per_page[dctkey]= page_content\n",
    "    os.remove('cropped_image.pdf')\n",
    "    os.remove('PDF_image.png')\n",
    "    # Closing the pdf file object\n",
    "    pdfFileObj.close()\n",
    "    return text_per_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bb5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def file_size(file_path):\n",
    "    \"\"\"\n",
    "    this function will return the file size\n",
    "    \"\"\"\n",
    "    if os.path.isfile(file_path):\n",
    "        file_info = os.stat(file_path)\n",
    "        return file_info.st_size/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tika import parser\n",
    "for f in os.listdir(\"./pdf/2023/\"):\n",
    "    print(f)\n",
    "    try:\n",
    "        path_pdf = \"./pdf/2023/\" + f\n",
    "        # Deleting the additional files created\n",
    "        text_per_page = parser.from_file(path_pdf)\n",
    "        name = f.replace(\"pdf\", \"txt\")\n",
    "        with open('txt/2023/tika_' + name, 'w') as file:\n",
    "            file.write(json.dumps(text_per_page[\"content\"]))\n",
    "\n",
    "        file_path = 'txt/2023/tika_' + name\n",
    "        size = file_size(file_path)\n",
    "        if size > 50:\n",
    "            os.remove(path_pdf)\n",
    "        else:\n",
    "            print(\"undeleted: \", path_pdf)\n",
    "            destination = \"./pdf/done/\" + f\n",
    "            os.rename(path_pdf, destination)\n",
    "    except:\n",
    "        print(\"failed: \", path_pdf)\n",
    "        destination = \"./pdf/failed/\" + f\n",
    "        os.rename(path_pdf, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5514b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2753a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect\n",
    "import ast\n",
    "def preprocess_text(text):\n",
    "    lst_sent = []\n",
    "    text = text.encode().decode('unicode-escape')\n",
    "    text = text.replace(\"-\\n\", \"\").split(\"\\n\\n\")\n",
    "\n",
    "    for sent in text:\n",
    "        if len(sent) > 0:\n",
    "            sent = sent.replace(\"\\n\", \" \")\n",
    "            numbe_per = sum([alph.isnumeric() for alph in sent])/len(sent)\n",
    "            if (numbe_per<0.9):\n",
    "                lst_sent.append(sent)\n",
    "            else:\n",
    "                pass\n",
    "    return lst_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67122586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta is the meta information collected from previous stage;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in meta.iterrows():\n",
    "    try:\n",
    "        file = \"./txt/2023/\" + row[\"pdf_id\"].split(\".\")[0] + \".txt\"\n",
    "        with open(file,\"r\") as f:\n",
    "            tmp = f.read()\n",
    "        #tmp = preprocess_text(tmp)\n",
    "    except:\n",
    "        try:\n",
    "            file = \"./txt/2023/\" + row[\"pdf_id\"].split(\".\")[0] + \"_tika.txt\"\n",
    "            with open(file,\"r\") as f:\n",
    "                tmp = f.read()\n",
    "            #tmp = preprocess_text(tmp)\n",
    "        except:\n",
    "            tmp = None\n",
    "    meta.loc[idx, \"text\"] =tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958ee423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
